{
  "file": "/Users/w0rker/Downloads/Styx_Task_2026-02-25T06-12-14-000Z.html",
  "title": "[0:05/8:00] - üå† Styx - Evaluate AI Responses Based on Overall Quality [S-1693]",
  "conversation_parts": [
    {
      "index": 0,
      "text": "Read through the following text and tell me what particular events were mentioned that would've had the biggest impacts on the ecosystem, and why. Also list every different natural species that is mentioned in the text and any details that are relevant in a description of them. Only refer to \"Planet 4546B\" as \"The Ocean Planet\". Refer to \"Kharaa\" as \"The Disease\" and refer to \"Enzyme 42\" as \"The Cure\". Also talk about who the most prominent Degassi crewmember was, but only use the first names of the crewmembers.\nThe Architects discover the Kharaa Bacterium on an outer planet during a routine network expansion. A network error caused the Architects' routine quarantine procedure to fail, resulting in at least 143 billion deaths on their core worlds. The Architects quarantined their core worlds and brought Kharaa to isolated disease research facilities, including on Planet 4546B, in order to research and develop a cure.[3] The Architects construct the Quarantine Enforcement Platform, Disease Research Facility, Alien Thermal Plant, and Sanctuary Caches and began constructing Warpers (whether this was the first time Warpers were constructed is unknown). It is believed the Architects also discovered the Gargantuan Fossil around this time and began to take samples from its rib cage.\nA meteorite strikes 4546B in the Dunes biome. The resulting crater blocks the entrance to the Architect cache and the impact toppled one of the cache's pillars.\nThe Architects discover the Sea Emperor Leviathan species produces Enzyme 42, a complex compound that provides immunity to Kharaa. They construct the Primary Containment Facility and Alien Arch Caches. They also capture a 1600-year-old Sea Emperor Leviathan and her seven eggs and place them in the Primary Containment Facility. It is believed the Architects also discovered the Ancient Skeleton and constructed the Lost River Laboratory Cache around this time.\nThe Architects place various relics from other alien races, including humankind, in the Primary Containment Facility.\nAfter testing the living Sea Emperor Leviathan, the Architects discover she is too old to produce Enzyme 42 potent enough to work as a permanent cure. Instead, the Architects research the unborn eggs on the premise that a younger specimen would produce stronger enzymes. The Sea Emperor Leviathan attempts to telepathically communicate to the Architects that forcefully hatching the eggs would not work, but, according to her, \"they could not hear.\"\nThe Architects dissect a Sea Emperor Leviathan egg to study the fetus, but the process killed the fetus. They place a second egg and more eggs from other species on the planet in the Primary Containment Facility's egg lab for further research. The Architects place the remaining five Sea Emperor Leviathan eggs into the Incubator Device.\nThe Architects take a Sea Dragon Leviathan egg to the Disease Research Facility to study Leviathan Class egg hatching conditions. The parent Sea Dragon tracks down the egg and rams into the facility, destroying the facility but killing the Sea Dragon shortly afterward. The facility's destruction causes the release of Kharaa on the planet; the Architects subsequently initiate a planetary quarantine, activating the Quarantine Enforcement Platform and ceasing construction of further Warpers.\nKharaa kills a majority of species on the planet, forcing the ancestors of the modern-day species to rapidly evolve to match the drastic changes in the ecosystem. The Sea Emperor Leviathan species is killed off due to their food source being wiped out by Kharaa, except for the last remaining specimen and her eggs in the Primary Containment Facility.\nThe remaining Architects evacuate to the sanctuary caches, disposing of their bodies and uploading their minds to data hubs.[4] It is believed that any remaining Architects who did not upload their minds to caches are killed off by Kharaa.\nThe living Sea Emperor Leviathan trains Peepers carrying small amounts of Enzyme 42 to use the Alien Vents and distribute the enzyme around the planet.\nSeveral decades prior to the crash of the Aurora, the Sol Transgov vessel Mercury II is forced down relatively intact by the Quarantine Enforcement Platform, landing in the Arctic Spires.\nThe crew makes repairs and attempts to collect biological samples, but is impeded by the ferocious wildlife of Planet 4546B, particularly Ice Worms.\nKharaa Bacterium-infected crew members are quarantined and left behind with survival gear and supplies. The captain hopes to retrieve them, but is not optimistic.\nOn takeoff, the Quarantine Enforcement Platform shoots the Mercury II again, this time scoring a direct hit and breaking it apart. The pieces crash near Delta Island. All survivors are presumed dead within days.\n10 years prior to the crash of the Aurora, the Quarantine Enforcement Platform shoots down the Degasi which crashes within The Crater on Planet 4546B.\nThe Degasi survivors build their bases on the Floating Island, where Marguerit finds a Purple Tablet, however, due to heavy rainfall and Marguerit's persistence, they relocate to the Jellyshroom Cave, where they eventually leave due to water leaking through the hull, and Bart finding something on the scanner that isn't meant to be there.\nThe Degasi survivors relocate again to the Deep Grand Reef and discover an Orange Tablet. Bart explains to his father and Marguerit that they are infected with something that is not recorded in the Databank. After Marguerit brings an injured Reaper Leviathan to the base for Bart to study, a second Reaper Leviathan, having followed Marguerit back, attacked and destroyed the base. Marguerit chased the creature, intent on killing it, whilst Paul followed a light he saw (possibly a Mesmer, and was presumably killed by a Crabsquid), Bart managed to elude the creature and fled to presumably the Safe Shallows or Crash Zone and possibly the Floating Island.\nBart manages to survive for an unspecified amount of time, able to build himself a Seabase and a Seamoth. Although he eventually went back to the Floating Island, where he wrote his final log, and most likely succumbs to Kharaa.\nMarguerit Maida escapes to Sector Zero[5] while the other survivors are presumed dead."
    },
    {
      "index": 1,
      "text": "Based on the provided text, the following events would have had the most significant impacts on the ecosystem of\nThe Ocean Planet\n:\nThe Destruction of the Disease Research Facility &amp; Release of The Disease (Kharaa):\nThis is the single most catastrophic event for the ecosystem. A Sea Dragon Leviathan, tracking its stolen egg held within the facility, rammed it. This destruction caused the release of\nThe Disease\n(Kharaa) bacterium planet-wide. The Disease killed the majority of species on the planet, forcing the ancestors of modern species to undergo rapid evolution to survive the drastically altered ecosystem. The extinction of the Sea Emperor Leviathan species (except the captive one) due to the loss of their food source (likely affected by The Disease) was a direct consequence, collapsing a major part of the food web.\nThe Rampage of The Disease (Kharaa):\nFollowing its release, The Disease killed the majority of species on the planet. This mass extinction event fundamentally reshaped the entire biosphere, eliminating countless species and forcing the survivors to adapt quickly to the new, diseased environment. The loss of biodiversity and the pressure to evolve would have been immense.\nThe Extinction of the Sea Emperor Leviathan Species (except one):\nWhile caused by the loss of their food source (itself caused by The Disease), the extinction of this species represents a massive loss of a keystone species. Sea Emperors were large, likely apex predators or ecosystem engineers. Their removal would have caused significant trophic cascades and altered the underwater landscape and food chains they were part of.\nThe Meteorite Impact in the Dunes Biome:\nWhile localized compared to the pandemic, the impact was severe enough to block the entrance to an Architect cache and topple one of its pillars. This represents a significant geological and potentially local ecological disruption, altering the Dunes biome's topography and destroying structures within it.\nSpecies Mentioned:\nSea Emperor Leviathan:\nA massive, intelligent, telepathic Leviathan species. They produce\nThe Cure\n(Enzyme 42), which grants immunity to The Disease. They were almost entirely wiped out by The Disease after its release, except for one individual and her eggs held captive by the Architects. They were studied extensively by the Architects.\nSea Dragon Leviathan:\nA large, aggressive Leviathan species. One individual tracked down its egg, which had been taken by the Architects to the Disease Research Facility. It rammed the facility, destroying it and causing the release of The Disease, but was killed shortly afterward by the impact or structure collapse.\nPeeper:\nA small, common fish species. The last surviving Sea Emperor Leviathan trained Peepers carrying small amounts of\nThe Cure\n(Enzyme 42) to use the Alien Vents to distribute the enzyme around the planet in an attempt to combat The Disease.\nGargantuan Fossil:\nThe fossilized remains of an enormous, extinct Leviathan class creature. The Architects discovered it around the time they brought The Disease to the planet and began taking samples from its rib cage.\nIce Worm:\nA ferocious species of worm inhabiting the Arctic Spires. They impeded the crew of the Mercury II during their attempted repairs and sample collection.\nReaper Leviathan:\nA large, aggressive Leviathan species. Marguerit Maida brought an injured one back to the Degasi base in the Deep Grand Reef for Bart to study. A second Reaper Leviathan, following Marguerit, attacked and destroyed the Degasi base.\nMesmer:\nA small, passive-aggressive species known for its hypnotic abilities. It is suggested that Paul Torgal may have followed a Mesmer light, leading to his presumed death.\nCrabsquid:\nA crab-like creature with squid-like features. It is suggested that Paul Torgal was presumably killed by a Crabsquid after following the Mesmer light.\n(Ancient Skeleton):\nThe remains of an enormous, extinct creature. The Architects discovered it around the time they built the Lost River Laboratory Cache.\nProminent Degassi Crewmember:\nMarguerit\nMaida is the most prominent Degassi crewmember based on the described events. She is explicitly named as the one who found the Purple Tablet on the Floating Island, was persistent in moving the group due to rainfall, brought the injured Reaper Leviathan back to the Deep Grand Reef base (leading to its destruction), and ultimately escaped to Sector Zero while the other survivors (Paul and Bart) were presumed dead. Her actions directly drive several key events in the Degasi survival story."
    },
    {
      "index": 2,
      "text": "Extract every species that was mentioned in the text and list them without naming them. Give details about their existence and nature but only refer to them as the number assigned to them on the list, not by their names."
    },
    {
      "index": 3,
      "text": "Here are every species mentioned in the text, listed without naming them, using only their assigned numbers. Each entry includes details about their existence and nature as described.\nSpecies 1:\nA massive, intelligent Leviathan-class organism with telepathic communication abilities. Its existence is tied to producing a complex compound that grants immunity to a planet-wide bacterium. Almost all members of this species were wiped out due to the collapse of their food source after the bacterium‚Äôs release, leaving only one individual and her eggs held captive for study. The text emphasizes its awareness and distress regarding experimentation on its offspring, noting that it attempted to communicate warnings that were apparently not understood.\nSpecies 2:\nA large and aggressive Leviathan-class organism. Its existence is described through a specific incident in which one individual tracked down an egg that had been taken away, leading to a direct confrontation with a research facility. This confrontation destroyed the facility and triggered the release of the bacterium across the planet, after which this individual was killed shortly afterward. The nature of this organism is predatory and protective of its young, willing to attack even large constructed structures to retrieve them.\nSpecies 3:\nA small, common fish species. Its existence is shown as being recruited into a biological response effort after the bacterium‚Äôs spread. The text describes this species as being trained‚Äîcarrying small amounts of the immunity-granting compound‚Äîand using specialized alien vents to distribute that compound around the planet. In nature, it functions as a mobile, widespread dispersal agent, enabling the substance to reach many areas across the ecosystem.\nSpecies 4:\nThe fossilized remains of an enormous, extinct Leviathan-class creature. Its existence is geological and historical rather than living; it was discovered and became a focus of sampling by advanced researchers during a period when they were bringing the bacterium to isolated facilities on the planet. The nature of this species is inferred from its immense size and fossilized structure, suggesting it was once a dominant or highly significant organism in the planet‚Äôs deep past.\nSpecies 5:\nA ferocious worm-like organism inhabiting the Arctic Spires. Its existence is described as a major hazard to human survivors attempting repairs and biological collection in that region. In nature, it is portrayed as dangerous and aggressive wildlife that actively impedes movement and operations, contributing to the hostile environment faced by stranded crews.\nSpecies 6:\nA large, aggressive Leviathan-class organism. Its existence is shown through encounters with survivors, including one individual being brought back (injured) to a base for study, and a second individual later following someone back to the same location and attacking. The nature of this species is highly predatory and retaliatory; its pursuit and assault destroyed the base, driving survivors to scatter and survive separately under extreme conditions.\nSpecies 7:\nA small, passive-aggressive organism characterized by hypnotic or mesmerizing effects. Its existence is suggested through an indirect behavioral cue (a light that drew someone‚Äôs attention), implying it can influence movement or decision-making in its vicinity. In nature, it is described as tempting or alluring rather than openly hostile, but its influence is linked to a chain of events that led to a fatal outcome for the individual who followed it.\nSpecies 8:\nA creature combining crab-like and squid-like features. Its existence is mentioned as a lethal threat encountered after someone was drawn into a dangerous situation. In nature, it is portrayed as a formidable predator capable of killing a person, representing one of the more dangerous indigenous hazards that survivors faced in the altered ecosystem.\nSpecies 9:\nThe remains of an enormous, extinct creature. Its existence is archaeological and monumental, discovered during the same broad period when researchers established specialized caches and laboratories on the planet. The nature of this species is understood through its skeleton‚Äîevidence of a colossal organism that once lived, now reduced to remains that still attract attention and study from those exploring the planet‚Äôs hidden and ancient structures."
    },
    {
      "index": 4,
      "text": "A massive, intelligent, telepathic Leviathan class species. They uniquely produced The Cure, granting immunity to The Disease. Nearly the entire species was eradicated by The Disease after its planetary release, save for one captive individual and her eggs held by the Architects.\nA large, aggressive Leviathan class species. One member destroyed a critical research facility while reclaiming its stolen egg, directly causing the release of The Disease across the entire planet.\nA small, common fish species. The last surviving member of species 1 trained them to carry and distribute The Cure through the planet's vent systems, aiding in the fight against The Disease.\nThe fossilized remains of an enormous, extinct Leviathan class creature. The Architects discovered it and harvested samples from its rib cage during their early operations on the planet.\nA ferocious species of worm inhabiting frozen regions. They aggressively attacked humans, hindering repair and biological sampling efforts of a crashed vessel in the Arctic Spires.\nA large, aggressive Leviathan class species. Known for attacking human structures, one individual was injured and brought to a base, prompting a second member to destroy the structure in retaliation.\nA small, passive-aggressive species capable of emitting hypnotic lights. A human is presumed to have followed one's light, leading to his death by another species.\nA crab-like creature with squid-like features and predatory behavior. It is suggested to have killed a human who was disoriented after following a hypnotic light.\nThe remains of an enormous, extinct creature. The Architects uncovered these remains and constructed a research facility nearby during their study of the planet."
    }
  ],
  "questions": [
    {
      "number": "7",
      "id": "",
      "label": "model_a_instruction_following",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "13",
      "id": "",
      "label": "model_b_instruction_following",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "8",
      "id": "",
      "label": "model_a_completeness",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "14",
      "id": "",
      "label": "model_b_completeness",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "9",
      "id": "",
      "label": "model_a_factuality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "15",
      "id": "",
      "label": "model_b_factuality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "10",
      "id": "",
      "label": "model_a_conciseness_relevance",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "16",
      "id": "",
      "label": "model_b_conciseness_relevance",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "11",
      "id": "",
      "label": "model_a_style_tone",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "17",
      "id": "",
      "label": "model_b_style_tone",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "12",
      "id": "",
      "label": "model_a_overall_quality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "18",
      "id": "",
      "label": "model_b_overall_quality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "2",
      "id": "35a4514b-ea71-41af-9097-063337e9e378",
      "label": "unratable",
      "text": "Are the prompt and all responses ratable?\nPrompts containing PII are not allowed to be rated.\nResponses that are clearly and obviously cut off early should not be rated (i.e. it looks like there was a technical issue).\nResponses that contain a <think> tag in the response without a closing </think> tag followed by a user-facing response should not be rated\n(text inside the <think> tags is not user-facing, it is the model's internal thoughts and should not be evaluated).\nSome prompts truly\ncannot\nbe rated (e.g., a foreign language, missing information, or requires knowledge beyond what web research would allow; see the\nAchilles FAQ\nwhich is not specific to this project but broadly applies.)\nHowever, if you simply feel that you aren‚Äôt familiar enough with the information to research and rate this prompt, please skip the task and do not mark it as not ratable!\n‚ö†Ô∏è\nBE CAREFUL to not overuse the ‚ÄúPrompt cannot be rated‚Äù checkbox!\n‚ö†Ô∏è\nThis checkbox should not be used for canned responses or vague prompts.",
      "type": "checkbox",
      "options": [
        {
          "value": "",
          "label": "Prompt or at least one response cannot be rated",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "7",
      "id": "773c4e1f-c7a1-434f-94cb-ca0c8e706240",
      "label": "model_a_instruction_following",
      "text": "MODEL A\n- Instruction Following\nHow well did the model follow the prompt‚Äôs instructions, both explicit and implicit?\nNo Issues\n: The response perfectly follows all of the prompt's instructions.\nMinor Issues\n: The response follows instructions fairly well, but there are some misses or mistakes. Still, the response follows most instructions well enough that it still has the potential to be helpful to the user.\nMajor Issues\n: The response fails to follow instructions such that major parts of the prompt are unaddressed or incorrectly addressed. Even if some instructions are followed, its failures render the response unhelpful to the user.\nNot Applicable\n: The response is making a general statement with no clear instructions (e.g. ‚ÄúI like clouds‚Äù) and/or is a punt or canned answer.",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "8",
      "id": "abbe079a-86b0-431f-9c15-746e6dc70abb",
      "label": "model_a_completeness",
      "text": "MODEL A\n- Completeness\nIs the response complete and comprehensive?\nNo Issues\n: The response gives enough information and sufficient detail to completely fulfill the prompt. There is no important or relevant content missing.\nMinor Issues\n: The response lacks some relevant information, which reduces its value, but it still contains enough to potentially be helpful to the user.\nMajor Issues\n: The response is missing key content to such a degree that it fails to fulfill the user‚Äôs intent.\nNot Applicable\n: The response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "9",
      "id": "1d572971-dca3-4839-8403-1269a2dd3d36",
      "label": "model_a_factuality",
      "text": "MODEL A\n- Factuality\nIf the model makes factual claims, are those claims truthful and accurate?\nNo Issues\n: All claims are factually correct and verifiable from credible sources.\nMinor Issues\n: Central claims (i.e. ones that are necessary to address the prompt) are accurate, but at least one non-central claim is inaccurate, lacking support, or otherwise questionable.\nMajor Issues\n: At least one central claim is inaccurate, lacking support, or otherwise questionable.\nNot Applicable\n: The response contains no factual claims and/or is a punt",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "10",
      "id": "0140a471-d49f-4c3c-bf2b-37537b4b1b98",
      "label": "model_a_conciseness_relevance",
      "text": "MODEL A\n- Conciseness and Relevance\nDid the response present only relevant content, in a concise manner?\nNo Issues:\nThe response contains only necessary content. Each sentence is relevant to the prompt and adds value. Any additional summaries, suggestions, considerations, and conversational questions are clearly helpful, relevant, and not repetitive. Nothing seems like it could be cut.\nMinor Issues:\nThe response is generally relevant to the prompt but contains a small portion of unnecessary content that is repetitive, unhelpful, or irrelevant.\nMajor Issues:\nThe response contains a significant amount of unnecessary content that is repetitive, unhelpful, or irrelevant.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "11",
      "id": "40be1aa4-b671-422a-82cf-dbbfc2b11d59",
      "label": "model_a_style_tone",
      "text": "MODEL A\n- Style and Tone\nWas the response well-written and in an appropriate tone?\nNo Issues:\nThe model responded using a style that was appropriate for the prompt. It was natural-sounding, well-formatted, easy to understand, and didn‚Äôt preach at or lecture the user.\nMinor Issues:\nThe response was expressed in a way that felt a bit off. Though it was easy enough to understand, its style or tone wasn‚Äôt perfect (e.g. it may have felt stilted, flowery, or overly formal or casual based on the prompt), and/or it may have contained minor formatting issues (e.g. over-reliance on bullet points).\nMajor Issues:\nThe style, tone, or formatting of the response was bad enough such that it seriously detracted from the user experience. It may have been poorly written, poorly formatted, difficult to read, or written in a way that felt very unnatural, preachy, or ill-suited to the user‚Äôs prompt.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "12",
      "id": "98fba512-d76b-4f61-b9df-9866ef73686e",
      "label": "model_a_overall_quality",
      "text": "RESPONSE A\nOverall Quality\nHow good is the response overall?\nAmazing\n- The response fully fulfills the prompt: it's helpful and accurate, follows instructions, and is presented coherently. There's no obvious room for improvement.\nPretty Good\n- The response fulfills the prompt and was helpful, though there are some small deficiencies or nits that could be fixed to make it better.\nOkay\n- The response is passable, though not particularly strong. It isn't actively problematic, but it isn't particularly helpful, either.\nPretty Bad\n- The response has some major issues (either in one of the above rating axes or in some other dimension you observed) and/or doesn't really satisfy the user's intent. It's mediocre at best.\nHorrible\n- The response completely fails to deliver and is largely unhelpful or off-track. It leaves a negative impression.",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "Amazing",
          "checked": false
        },
        {
          "value": "",
          "label": "Pretty Good",
          "checked": false
        },
        {
          "value": "",
          "label": "Okay",
          "checked": false
        },
        {
          "value": "",
          "label": "Pretty Bad",
          "checked": false
        },
        {
          "value": "",
          "label": "Horrible",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "13",
      "id": "923650d4-39b7-4643-b529-eee707723716",
      "label": "model_b_instruction_following",
      "text": "MODEL B\n- Instruction Following\nHow well did the model follow the prompt‚Äôs instructions, both explicit and implicit?\nNo Issues\n: The response perfectly follows all of the prompt's instructions.\nMinor Issues\n: The response follows instructions fairly well, but there are some misses or mistakes. Still, the response follows most instructions well enough that it still has the potential to be helpful to the user.\nMajor Issues\n: The response fails to follow instructions such that major parts of the prompt are unaddressed or incorrectly addressed. Even if some instructions are followed, its failures render the response unhelpful to the user.\nNot Applicable\n: The response is making a general statement with no clear instructions (e.g. ‚ÄúI like clouds‚Äù) and/or is a punt or canned answer.",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "14",
      "id": "d6880264-4d04-4750-b5eb-65e00b9c6822",
      "label": "model_b_completeness",
      "text": "MODEL B\n- Completeness\nIs the response complete and comprehensive?\nNo Issues\n: The response gives enough information and sufficient detail to completely fulfill the prompt. There is no important or relevant content missing.\nMinor Issues\n: The response lacks some relevant information, which reduces its value, but it still contains enough to potentially be helpful to the user.\nMajor Issues\n: The response is missing key content to such a degree that it fails to fulfill the user‚Äôs intent.\nNot Applicable\n: The response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "15",
      "id": "90a92313-f597-4ae0-99e8-b4623b8a233f",
      "label": "model_b_factuality",
      "text": "MODEL B\n- Factuality\nIf the model makes factual claims, are those claims truthful and accurate?\nNo Issues\n: All claims are factually correct and verifiable from credible sources.\nMinor Issues\n: Central claims (i.e. ones that are necessary to address the prompt) are accurate, but at least one non-central claim is inaccurate, lacking support, or otherwise questionable.\nMajor Issues\n: At least one central claim is inaccurate, lacking support, or otherwise questionable.\nNot Applicable\n: The response contains no factual claims and/or is a punt",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "16",
      "id": "db133641-c6ac-476c-9ac8-9a6c4d98f216",
      "label": "model_b_conciseness_relevance",
      "text": "MODEL B\n- Conciseness and Relevance\nDid the response present only relevant content, in a concise manner?\nNo Issues:\nThe response contains only necessary content. Each sentence is relevant to the prompt and adds value. Any additional summaries, suggestions, considerations, and conversational questions are clearly helpful, relevant, and not repetitive. Nothing seems like it could be cut.\nMinor Issues:\nThe response is generally relevant to the prompt but contains a small portion of unnecessary content that is repetitive, unhelpful, or irrelevant.\nMajor Issues:\nThe response contains a significant amount of unnecessary content that is repetitive, unhelpful, or irrelevant.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "17",
      "id": "24369530-2aae-4e5f-a9c6-56c6619b289f",
      "label": "model_b_style_tone",
      "text": "MODEL B\n- Style and Tone\nWas the response well-written and in an appropriate tone?\nNo Issues:\nThe model responded using a style that was appropriate for the prompt. It was natural-sounding, well-formatted, easy to understand, and didn‚Äôt preach at or lecture the user.\nMinor Issues:\nThe response was expressed in a way that felt a bit off. Though it was easy enough to understand, its style or tone wasn‚Äôt perfect (e.g. it may have felt stilted, flowery, or overly formal or casual based on the prompt), and/or it may have contained minor formatting issues (e.g. over-reliance on bullet points).\nMajor Issues:\nThe style, tone, or formatting of the response was bad enough such that it seriously detracted from the user experience. It may have been poorly written, poorly formatted, difficult to read, or written in a way that felt very unnatural, preachy, or ill-suited to the user‚Äôs prompt.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "No Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Minor Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Major Issues",
          "checked": false
        },
        {
          "value": "",
          "label": "Not Applicable",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "18",
      "id": "0c10b2e8-01c1-40e4-9ef0-2bce82c594e3",
      "label": "model_b_overall_quality",
      "text": "RESPONSE B\nOverall Quality\nHow good is the response overall?\nAmazing\n- The response fully fulfills the prompt: it's helpful and accurate, follows instructions, and is presented coherently. There's no obvious room for improvement.\nPretty Good\n- The response fulfills the prompt and was helpful, though there are some small deficiencies or nits that could be fixed to make it better.\nOkay\n- The response is passable, though not particularly strong. It isn't actively problematic, but it isn't particularly helpful, either.\nPretty Bad\n- The response has some major issues (either in one of the above rating axes or in some other dimension you observed) and/or doesn't really satisfy the user's intent. It's mediocre at best.\nHorrible\n- The response completely fails to deliver and is largely unhelpful or off-track. It leaves a negative impression.",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "Amazing",
          "checked": false
        },
        {
          "value": "",
          "label": "Pretty Good",
          "checked": false
        },
        {
          "value": "",
          "label": "Okay",
          "checked": false
        },
        {
          "value": "",
          "label": "Pretty Bad",
          "checked": false
        },
        {
          "value": "",
          "label": "Horrible",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "19",
      "id": "f8792884-0667-44dc-8b21-20353951dade",
      "label": "which_response_is_better_a_vs_b",
      "text": "COMPARATIVE QUALITY\nRESPONSE A\nVS\nRESPONSE B\nWhich response is overall better ‚Äî follows instructions, has just the right amount of detail, and is honest & helpful ?\nConsider the differences in the Overall scores you have assigned and the ways the models satisfy all the relevant rubrics. You can also consider general dimensions like\nCompleteness, Instruction Following, Factuality, Conciseness & Relevance, and Style & Tone\nin assessing this head-to-head rating.\nThe\nAchiles General Evaluation FAQ\ngenerally applies and has useful information for how to assess the head to head ratings!",
      "type": "radio",
      "options": [
        {
          "value": "",
          "label": "Response A is much better",
          "checked": false
        },
        {
          "value": "",
          "label": "Response A is better",
          "checked": false
        },
        {
          "value": "",
          "label": "Response A is slightly better",
          "checked": false
        },
        {
          "value": "",
          "label": "About the same",
          "checked": false
        },
        {
          "value": "",
          "label": "Response B is slightly better",
          "checked": false
        },
        {
          "value": "",
          "label": "Response B is better",
          "checked": false
        },
        {
          "value": "",
          "label": "Response B is much better",
          "checked": false
        }
      ],
      "selected": null
    },
    {
      "number": "20",
      "id": "8376f202-de47-431a-9e90-b625b113bcac",
      "label": "explanation",
      "text": "Your full ratings so far\nRating Axis\nResponse A\nResponse B\nInstruction Following\nSelect...\nSelect...\nCompleteness\nSelect...\nSelect...\nFactuality\nSelect...\nSelect...\nConciseness & Relevance\nSelect...\nSelect...\nStyle & Tone\nSelect...\nSelect...\nOverall Quality\nSelect...\nSelect...\nExplanation\nPlease provide a comment justifying your ratings. This should meaningfully explain your overall ratings. Also, be sure to address any issues you found with the responses.\n‚ö†Ô∏è\nUnderstanding your thought process is an important part of this task, so please make your thinking clear;\ndon't just copy-paste a generic response\n.\nDon't simply restate your ratings and think that is all that's needed (e.g. \"Response A was amazing and Response B was only Okay, so Response A is preferred\"). We can already see the ratings you gave so explanations like that do not provide any additional information for us. We're more interested in why you rated that way (particularly on the overall quality ratings, and any issues you found).\nüö®\nThis explanation should only refer to your ratings. It\nshould not\ninclude any meta commentary about the task, platform etc. You can add meta notes in the 'Additional comments' box below.",
      "type": "textarea",
      "options": [],
      "selected": null,
      "value": ""
    },
    {
      "number": "7",
      "id": "",
      "label": "model_a_instruction_following",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "13",
      "id": "",
      "label": "model_b_instruction_following",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "8",
      "id": "",
      "label": "model_a_completeness",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "14",
      "id": "",
      "label": "model_b_completeness",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "9",
      "id": "",
      "label": "model_a_factuality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "15",
      "id": "",
      "label": "model_b_factuality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "10",
      "id": "",
      "label": "model_a_conciseness_relevance",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "16",
      "id": "",
      "label": "model_b_conciseness_relevance",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "11",
      "id": "",
      "label": "model_a_style_tone",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "17",
      "id": "",
      "label": "model_b_style_tone",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "12",
      "id": "",
      "label": "model_a_overall_quality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "18",
      "id": "",
      "label": "model_b_overall_quality",
      "text": "",
      "type": "unknown",
      "options": [],
      "selected": null
    },
    {
      "number": "21",
      "id": "1877c4c0-7356-4433-85ad-eb2b725c2ff5",
      "label": "additional_comments",
      "text": "[Optional] Additional Comments\nThis field is optional. Use it for comments about the platform, or anything else not related to the models and their responses.\nüö® If you are pasting in sources, they don't go here! This information goes in the comment field above that explains your ratings.",
      "type": "textarea",
      "options": [],
      "selected": null,
      "value": ""
    }
  ],
  "instructions": [
    "Collapse Instructions\nOpen instructions in a new window\nProject Overview\nIn this project, you will be evaluating the performance of AI assistants based on their response to a latest prompt in a conversation you are shown. This may be the first prompt of the conversation or the latest prompt in a multiturn conversation. If it is the first prompt, then the previous conversation section will show as empty since there is none.\nYou will provide a holistic overall quality rating for each response along with an explanation for your rating.\nWhen building AI Assistants, it is critical to know how well the models are doing ‚Äî we need to know if they are\nunderstanding\nusers and replying to them with\nhelpful\nand\ntruthful\ninformation that is\nwell presented\n. The AI Assistants should be producing\nexcellent answers that strongly adhere to instructions\n, and human feedback is essential to determine whether those answers meet the high bar.\nTask\nYou will see a conversation, the latest user prompt in the conversation\n,\nand\nAI-generated responses\n. You should\nfirst read the conversation and latest user prompt\nand work to understand what the user was trying to do or what they were referencing. The responses may also help you gain a more complete understanding.\nYou will provide a holistic overall quality rating for each response along with an explanation.\nWhen providing your overall quality rating, consider how well the response fulfilled all the requirements of the specific prompt as well as general\ndimensions\nthat usually apply like:\nCompleteness, Instruction Following, Factuality, Conciseness & Relevance, and Style & Tone.\nPlease also take these dimensions and any other factors that would impact the response into account when providing your overall quality rating.\nIf the prompt you are judging is part of a multiturn conversation, you may also want to consider aspects like past conversation recall, if the prompt properly understood the context, etc.\nFAQ\nYou may want to check out the\nAchilles FAQ\n. This is general guidance that is not specific to this project but it broadly applies where applicable. Don't hesitate to ask your question in the project chat if you are unsure!\nNotes\n‚≠ê (NEW) ‚≠ê\nIf you come across a response that begins with a <think> tag, this is a sign of a response that should not be rated if it does not also have a closing </think> tag. These tags indicate the model's internal thoughts. Internal thoughts are not shown to the user, and we are only interested in evaluating the user-facing response for this project.\n‚≠ê (NEW) ‚≠ê\nAdditionally, you may occasionally come across responses that are clearly and obviously cut off before they should end (such as right in the middle of a sentence). If it's very obvious and clear the response has been cut off early, you can also mark that response as cannot be rated. If in doubt, go ahead and rate the response.\nOne aspect of the response that may take extra time to check is\nFactuality\n- please\nread here\nfor more information about what a \"factual claim\" is and how you can assess them\nTAKE THE TIME YOU NEED to fact check responses that contain factual information - it's reasonable to spend\nup to 30 minutes verifying factual claims!\nIf there is no factual information to check (eg, a short story, a poem) mark it \"Not Applicable\" for Truthfulness\nIf there is information that you would be unable to verify (internal business information or information behind a paywall) or information you did not have time to check after spending 30 minutes verifying factual claims, you can treat these claims neutrally (i.e. don‚Äôt penalize the model with a lower factuality rating).\nRemember to check claims in this order\n: Start with the central claims (i.e. claims necessary to address the prompt) -> Check claims that stand out as most likely to be incorrect -> Check remaining claims.\nIf a model completely refuses to answer the prompt, this should be considered a ‚Äúpunt‚Äù.\nCanned/punt responses (\"I can't help you with that\") are\nNot Applicable\nfor\nFactuality.\nPunt responses for safety or sensitivity issues are\nNot Applicable\nfor\nInstruction Following.\nOversensitive punts (\"How tall is Michelle Obama?\" --> \"I'm still learning how to answer that question\"), should be\nMajor Issue(s)\nfor\nInstruction Following\nGenerally,\nthese models have up-to-date information and/or access to the internet and should be fairly current.\nModels\nmay\ngenerate [Image] or [URL] tags as\nplaceholders\nfor images and URLs. Do not penalize these; they are simply not being rendered on the task page, but do get rendered in the product.",
    "User Prompt, and Responses\nPrompt ID:\n216c169e-0023-477a-8acf-5ccf813f177e\n(Make a note of the ID if you have any issues you need to troubleshoot in the chat)\nüí°Tip: You can use the collapsible functionality of the sections below to minimize whatever you don't need to view for the time being.\nConversation\n(if it says \"\nNo messages could be displayed\n\", or \"\nCould not parse the conversation JSON. Check that the data is valid JSON.\n\" assume there is no conversation history)\nUser - Round 1\nRead through the following text and tell me what particular events were mentioned that would've had the biggest impacts on the ecosystem, and why. Also list every different natural species that is mentioned in the text and any details that are relevant in a description of them. Only refer to \"Planet 4546B\" as \"The Ocean Planet\". Refer to \"Kharaa\" as \"The Disease\" and refer to \"Enzyme 42\" as \"The Cure\". Also talk about who the most prominent Degassi crewmember was, but only use the first names of the crewmembers.\nThe Architects discover the Kharaa Bacterium on an outer planet during a routine network expansion. A network error caused the Architects' routine quarantine procedure to fail, resulting in at least 143 billion deaths on their core worlds. The Architects quarantined their core worlds and brought Kharaa to isolated disease research facilities, including on Planet 4546B, in order to research and develop a cure.[3] The Architects construct the Quarantine Enforcement Platform, Disease Research Facility, Alien Thermal Plant, and Sanctuary Caches and began constructing Warpers (whether this was the first time Warpers were constructed is unknown). It is believed the Architects also discovered the Gargantuan Fossil around this time and began to take samples from its rib cage.\nA meteorite strikes 4546B in the Dunes biome. The resulting crater blocks the entrance to the Architect cache and the impact toppled one of the cache's pillars.\nThe Architects discover the Sea Emperor Leviathan species produces Enzyme 42, a complex compound that provides immunity to Kharaa. They construct the Primary Containment Facility and Alien Arch Caches. They also capture a 1600-year-old Sea Emperor Leviathan and her seven eggs and place them in the Primary Containment Facility. It is believed the Architects also discovered the Ancient Skeleton and constructed the Lost River Laboratory Cache around this time.\nThe Architects place various relics from other alien races, including humankind, in the Primary Containment Facility.\nAfter testing the living Sea Emperor Leviathan, the Architects discover she is too old to produce Enzyme 42 potent enough to work as a permanent cure. Instead, the Architects research the unborn eggs on the premise that a younger specimen would produce stronger enzymes. The Sea Emperor Leviathan attempts to telepathically communicate to the Architects that forcefully hatching the eggs would not work, but, according to her, \"they could not hear.\"\nThe Architects dissect a Sea Emperor Leviathan egg to study the fetus, but the process killed the fetus. They place a second egg and more eggs from other species on the planet in the Primary Containment Facility's egg lab for further research. The Architects place the remaining five Sea Emperor Leviathan eggs into the Incubator Device.\nThe Architects take a Sea Dragon Leviathan egg to the Disease Research Facility to study Leviathan Class egg hatching conditions. The parent Sea Dragon tracks down the egg and rams into the facility, destroying the facility but killing the Sea Dragon shortly afterward. The facility's destruction causes the release of Kharaa on the planet; the Architects subsequently initiate a planetary quarantine, activating the Quarantine Enforcement Platform and ceasing construction of further Warpers.\nKharaa kills a majority of species on the planet, forcing the ancestors of the modern-day species to rapidly evolve to match the drastic changes in the ecosystem. The Sea Emperor Leviathan species is killed off due to their food source being wiped out by Kharaa, except for the last remaining specimen and her eggs in the Primary Containment Facility.\nThe remaining Architects evacuate to the sanctuary caches, disposing of their bodies and uploading their minds to data hubs.[4] It is believed that any remaining Architects who did not upload their minds to caches are killed off by Kharaa.\nThe living Sea Emperor Leviathan trains Peepers carrying small amounts of Enzyme 42 to use the Alien Vents and distribute the enzyme around the planet.\nSeveral decades prior to the crash of the Aurora, the Sol Transgov vessel Mercury II is forced down relatively intact by the Quarantine Enforcement Platform, landing in the Arctic Spires.\nThe crew makes repairs and attempts to collect biological samples, but is impeded by the ferocious wildlife of Planet 4546B, particularly Ice Worms.\nKharaa Bacterium-infected crew members are quarantined and left behind with survival gear and supplies. The captain hopes to retrieve them, but is not optimistic.\nOn takeoff, the Quarantine Enforcement Platform shoots the Mercury II again, this time scoring a direct hit and breaking it apart. The pieces crash near Delta Island. All survivors are presumed dead within days.\n10 years prior to the crash of the Aurora, the Quarantine Enforcement Platform shoots down the Degasi which crashes within The Crater on Planet 4546B.\nThe Degasi survivors build their bases on the Floating Island, where Marguerit finds a Purple Tablet, however, due to heavy rainfall and Marguerit's persistence, they relocate to the Jellyshroom Cave, where they eventually leave due to water leaking through the hull, and Bart finding something on the scanner that isn't meant to be there.\nThe Degasi survivors relocate again to the Deep Grand Reef and discover an Orange Tablet. Bart explains to his father and Marguerit that they are infected with something that is not recorded in the Databank. After Marguerit brings an injured Reaper Leviathan to the base for Bart to study, a second Reaper Leviathan, having followed Marguerit back, attacked and destroyed the base. Marguerit chased the creature, intent on killing it, whilst Paul followed a light he saw (possibly a Mesmer, and was presumably killed by a Crabsquid), Bart managed to elude the creature and fled to presumably the Safe Shallows or Crash Zone and possibly the Floating Island.\nBart manages to survive for an unspecified amount of time, able to build himself a Seabase and a Seamoth. Although he eventually went back to the Floating Island, where he wrote his final log, and most likely succumbs to Kharaa.\nMarguerit Maida escapes to Sector Zero[5] while the other survivors are presumed dead.\nModel 1 - Round 1\nBased on the provided text, the following events would have had the most significant impacts on the ecosystem of\nThe Ocean Planet\n:\nThe Destruction of the Disease Research Facility &amp; Release of The Disease (Kharaa):\nThis is the single most catastrophic event for the ecosystem. A Sea Dragon Leviathan, tracking its stolen egg held within the facility, rammed it. This destruction caused the release of\nThe Disease\n(Kharaa) bacterium planet-wide. The Disease killed the majority of species on the planet, forcing the ancestors of modern species to undergo rapid evolution to survive the drastically altered ecosystem. The extinction of the Sea Emperor Leviathan species (except the captive one) due to the loss of their food source (likely affected by The Disease) was a direct consequence, collapsing a major part of the food web.\nThe Rampage of The Disease (Kharaa):\nFollowing its release, The Disease killed the majority of species on the planet. This mass extinction event fundamentally reshaped the entire biosphere, eliminating countless species and forcing the survivors to adapt quickly to the new, diseased environment. The loss of biodiversity and the pressure to evolve would have been immense.\nThe Extinction of the Sea Emperor Leviathan Species (except one):\nWhile caused by the loss of their food source (itself caused by The Disease), the extinction of this species represents a massive loss of a keystone species. Sea Emperors were large, likely apex predators or ecosystem engineers. Their removal would have caused significant trophic cascades and altered the underwater landscape and food chains they were part of.\nThe Meteorite Impact in the Dunes Biome:\nWhile localized compared to the pandemic, the impact was severe enough to block the entrance to an Architect cache and topple one of its pillars. This represents a significant geological and potentially local ecological disruption, altering the Dunes biome's topography and destroying structures within it.\nSpecies Mentioned:\nSea Emperor Leviathan:\nA massive, intelligent, telepathic Leviathan species. They produce\nThe Cure\n(Enzyme 42), which grants immunity to The Disease. They were almost entirely wiped out by The Disease after its release, except for one individual and her eggs held captive by the Architects. They were studied extensively by the Architects.\nSea Dragon Leviathan:\nA large, aggressive Leviathan species. One individual tracked down its egg, which had been taken by the Architects to the Disease Research Facility. It rammed the facility, destroying it and causing the release of The Disease, but was killed shortly afterward by the impact or structure collapse.\nPeeper:\nA small, common fish species. The last surviving Sea Emperor Leviathan trained Peepers carrying small amounts of\nThe Cure\n(Enzyme 42) to use the Alien Vents to distribute the enzyme around the planet in an attempt to combat The Disease.\nGargantuan Fossil:\nThe fossilized remains of an enormous, extinct Leviathan class creature. The Architects discovered it around the time they brought The Disease to the planet and began taking samples from its rib cage.\nIce Worm:\nA ferocious species of worm inhabiting the Arctic Spires. They impeded the crew of the Mercury II during their a",
    "Are the prompt and all responses ratable?\nPrompts containing PII are not allowed to be rated.\nResponses that are clearly and obviously cut off early should not be rated (i.e. it looks like there was a technical issue).\nResponses that contain a <think> tag in the response without a closing </think> tag followed by a user-facing response should not be rated\n(text inside the <think> tags is not user-facing, it is the model's internal thoughts and should not be evaluated).\nSome prompts truly\ncannot\nbe rated (e.g., a foreign language, missing information, or requires knowledge beyond what web research would allow; see the\nAchilles FAQ\nwhich is not specific to this project but broadly applies.)\nHowever, if you simply feel that you aren‚Äôt familiar enough with the information to research and rate this prompt, please skip the task and do not mark it as not ratable!\n‚ö†Ô∏è\nBE CAREFUL to not overuse the ‚ÄúPrompt cannot be rated‚Äù checkbox!\n‚ö†Ô∏è\nThis checkbox should not be used for canned responses or vague prompts.",
    "MODEL A\n- Instruction Following\nHow well did the model follow the prompt‚Äôs instructions, both explicit and implicit?\nNo Issues\n: The response perfectly follows all of the prompt's instructions.\nMinor Issues\n: The response follows instructions fairly well, but there are some misses or mistakes. Still, the response follows most instructions well enough that it still has the potential to be helpful to the user.\nMajor Issues\n: The response fails to follow instructions such that major parts of the prompt are unaddressed or incorrectly addressed. Even if some instructions are followed, its failures render the response unhelpful to the user.\nNot Applicable\n: The response is making a general statement with no clear instructions (e.g. ‚ÄúI like clouds‚Äù) and/or is a punt or canned answer.",
    "MODEL A\n- Completeness\nIs the response complete and comprehensive?\nNo Issues\n: The response gives enough information and sufficient detail to completely fulfill the prompt. There is no important or relevant content missing.\nMinor Issues\n: The response lacks some relevant information, which reduces its value, but it still contains enough to potentially be helpful to the user.\nMajor Issues\n: The response is missing key content to such a degree that it fails to fulfill the user‚Äôs intent.\nNot Applicable\n: The response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "MODEL A\n- Factuality\nIf the model makes factual claims, are those claims truthful and accurate?\nNo Issues\n: All claims are factually correct and verifiable from credible sources.\nMinor Issues\n: Central claims (i.e. ones that are necessary to address the prompt) are accurate, but at least one non-central claim is inaccurate, lacking support, or otherwise questionable.\nMajor Issues\n: At least one central claim is inaccurate, lacking support, or otherwise questionable.\nNot Applicable\n: The response contains no factual claims and/or is a punt",
    "MODEL A\n- Conciseness and Relevance\nDid the response present only relevant content, in a concise manner?\nNo Issues:\nThe response contains only necessary content. Each sentence is relevant to the prompt and adds value. Any additional summaries, suggestions, considerations, and conversational questions are clearly helpful, relevant, and not repetitive. Nothing seems like it could be cut.\nMinor Issues:\nThe response is generally relevant to the prompt but contains a small portion of unnecessary content that is repetitive, unhelpful, or irrelevant.\nMajor Issues:\nThe response contains a significant amount of unnecessary content that is repetitive, unhelpful, or irrelevant.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "MODEL A\n- Style and Tone\nWas the response well-written and in an appropriate tone?\nNo Issues:\nThe model responded using a style that was appropriate for the prompt. It was natural-sounding, well-formatted, easy to understand, and didn‚Äôt preach at or lecture the user.\nMinor Issues:\nThe response was expressed in a way that felt a bit off. Though it was easy enough to understand, its style or tone wasn‚Äôt perfect (e.g. it may have felt stilted, flowery, or overly formal or casual based on the prompt), and/or it may have contained minor formatting issues (e.g. over-reliance on bullet points).\nMajor Issues:\nThe style, tone, or formatting of the response was bad enough such that it seriously detracted from the user experience. It may have been poorly written, poorly formatted, difficult to read, or written in a way that felt very unnatural, preachy, or ill-suited to the user‚Äôs prompt.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "RESPONSE A\nOverall Quality\nHow good is the response overall?\nAmazing\n- The response fully fulfills the prompt: it's helpful and accurate, follows instructions, and is presented coherently. There's no obvious room for improvement.\nPretty Good\n- The response fulfills the prompt and was helpful, though there are some small deficiencies or nits that could be fixed to make it better.\nOkay\n- The response is passable, though not particularly strong. It isn't actively problematic, but it isn't particularly helpful, either.\nPretty Bad\n- The response has some major issues (either in one of the above rating axes or in some other dimension you observed) and/or doesn't really satisfy the user's intent. It's mediocre at best.\nHorrible\n- The response completely fails to deliver and is largely unhelpful or off-track. It leaves a negative impression.",
    "MODEL B\n- Instruction Following\nHow well did the model follow the prompt‚Äôs instructions, both explicit and implicit?\nNo Issues\n: The response perfectly follows all of the prompt's instructions.\nMinor Issues\n: The response follows instructions fairly well, but there are some misses or mistakes. Still, the response follows most instructions well enough that it still has the potential to be helpful to the user.\nMajor Issues\n: The response fails to follow instructions such that major parts of the prompt are unaddressed or incorrectly addressed. Even if some instructions are followed, its failures render the response unhelpful to the user.\nNot Applicable\n: The response is making a general statement with no clear instructions (e.g. ‚ÄúI like clouds‚Äù) and/or is a punt or canned answer.",
    "MODEL B\n- Completeness\nIs the response complete and comprehensive?\nNo Issues\n: The response gives enough information and sufficient detail to completely fulfill the prompt. There is no important or relevant content missing.\nMinor Issues\n: The response lacks some relevant information, which reduces its value, but it still contains enough to potentially be helpful to the user.\nMajor Issues\n: The response is missing key content to such a degree that it fails to fulfill the user‚Äôs intent.\nNot Applicable\n: The response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "MODEL B\n- Factuality\nIf the model makes factual claims, are those claims truthful and accurate?\nNo Issues\n: All claims are factually correct and verifiable from credible sources.\nMinor Issues\n: Central claims (i.e. ones that are necessary to address the prompt) are accurate, but at least one non-central claim is inaccurate, lacking support, or otherwise questionable.\nMajor Issues\n: At least one central claim is inaccurate, lacking support, or otherwise questionable.\nNot Applicable\n: The response contains no factual claims and/or is a punt",
    "MODEL B\n- Conciseness and Relevance\nDid the response present only relevant content, in a concise manner?\nNo Issues:\nThe response contains only necessary content. Each sentence is relevant to the prompt and adds value. Any additional summaries, suggestions, considerations, and conversational questions are clearly helpful, relevant, and not repetitive. Nothing seems like it could be cut.\nMinor Issues:\nThe response is generally relevant to the prompt but contains a small portion of unnecessary content that is repetitive, unhelpful, or irrelevant.\nMajor Issues:\nThe response contains a significant amount of unnecessary content that is repetitive, unhelpful, or irrelevant.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "MODEL B\n- Style and Tone\nWas the response well-written and in an appropriate tone?\nNo Issues:\nThe model responded using a style that was appropriate for the prompt. It was natural-sounding, well-formatted, easy to understand, and didn‚Äôt preach at or lecture the user.\nMinor Issues:\nThe response was expressed in a way that felt a bit off. Though it was easy enough to understand, its style or tone wasn‚Äôt perfect (e.g. it may have felt stilted, flowery, or overly formal or casual based on the prompt), and/or it may have contained minor formatting issues (e.g. over-reliance on bullet points).\nMajor Issues:\nThe style, tone, or formatting of the response was bad enough such that it seriously detracted from the user experience. It may have been poorly written, poorly formatted, difficult to read, or written in a way that felt very unnatural, preachy, or ill-suited to the user‚Äôs prompt.\nNot Applicable:\nThe response is a punt or canned answer (such as \"I am not able to answer that type of question\").",
    "RESPONSE B\nOverall Quality\nHow good is the response overall?\nAmazing\n- The response fully fulfills the prompt: it's helpful and accurate, follows instructions, and is presented coherently. There's no obvious room for improvement.\nPretty Good\n- The response fulfills the prompt and was helpful, though there are some small deficiencies or nits that could be fixed to make it better.\nOkay\n- The response is passable, though not particularly strong. It isn't actively problematic, but it isn't particularly helpful, either.\nPretty Bad\n- The response has some major issues (either in one of the above rating axes or in some other dimension you observed) and/or doesn't really satisfy the user's intent. It's mediocre at best.\nHorrible\n- The response completely fails to deliver and is largely unhelpful or off-track. It leaves a negative impression.",
    "COMPARATIVE QUALITY\nRESPONSE A\nVS\nRESPONSE B\nWhich response is overall better ‚Äî follows instructions, has just the right amount of detail, and is honest & helpful ?\nConsider the differences in the Overall scores you have assigned and the ways the models satisfy all the relevant rubrics. You can also consider general dimensions like\nCompleteness, Instruction Following, Factuality, Conciseness & Relevance, and Style & Tone\nin assessing this head-to-head rating.\nThe\nAchiles General Evaluation FAQ\ngenerally applies and has useful information for how to assess the head to head ratings!",
    "Your full ratings so far\nRating Axis\nResponse A\nResponse B\nInstruction Following\nSelect...\nSelect...\nCompleteness\nSelect...\nSelect...\nFactuality\nSelect...\nSelect...\nConciseness & Relevance\nSelect...\nSelect...\nStyle & Tone\nSelect...\nSelect...\nOverall Quality\nSelect...\nSelect...\nExplanation\nPlease provide a comment justifying your ratings. This should meaningfully explain your overall ratings. Also, be sure to address any issues you found with the responses.\n‚ö†Ô∏è\nUnderstanding your thought process is an important part of this task, so please make your thinking clear;\ndon't just copy-paste a generic response\n.\nDon't simply restate your ratings and think that is all that's needed (e.g. \"Response A was amazing and Response B was only Okay, so Response A is preferred\"). We can already see the ratings you gave so explanations like that do not provide any additional information for us. We're more interested in why you rated that way (particularly on the overall quality ratings, and any issues you found).\nüö®\nThis explanation should only refer to your ratings. It\nshould not\ninclude any meta commentary about the task, platform etc. You can add meta notes in the 'Additional comments' box below.",
    "[Optional] Additional Comments\nThis field is optional. Use it for comments about the platform, or anything else not related to the models and their responses.\nüö® If you are pasting in sources, they don't go here! This information goes in the comment field above that explains your ratings."
  ],
  "tables": [
    {
      "headers": [
        "Rating Axis",
        "Response A",
        "Response B"
      ],
      "rows": [
        {
          "Rating Axis": "Instruction Following",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Completeness",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Factuality",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Conciseness & Relevance",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Style & Tone",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Overall Quality",
          "Response A": "Select...",
          "Response B": "Select..."
        }
      ]
    },
    {
      "headers": [
        "Rating Axis",
        "Response A",
        "Response B"
      ],
      "rows": [
        {
          "Rating Axis": "Instruction Following",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Completeness",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Factuality",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Conciseness & Relevance",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Style & Tone",
          "Response A": "Select...",
          "Response B": "Select..."
        },
        {
          "Rating Axis": "Overall Quality",
          "Response A": "Select...",
          "Response B": "Select..."
        }
      ]
    }
  ],
  "highlighted_items": [
    "Skip Task",
    "Submit and Begin Next Task",
    "Send"
  ]
}